{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero tenemos que importar las librerías de las funciones que ocuparemos en el programa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import operator,functools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiremos una función la cual tendrá nos servirá para leer el archivo JSON, una vez que extraemos los datos del archivo, retornaremos una lista que contendrá todas las cadenas encontradas en el campo “short_descriotion”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json():\n",
    "    with open(\"News_Category_Dataset_v3.json\") as jsonFile:\n",
    "        jsonObject = json.load(jsonFile)\n",
    "    return [i['short_description'] for i in jsonObject]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También definiremos una función que nos servirá para leer el archivo txt que contiene las “stop words”, de igual manera retornaremos una lista que contenga las palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stopwords():\n",
    "    with open(\"stopwords-en.txt\", encoding=\"utf8\") as textfile:\n",
    "        data = [i.replace('\\n','') for i in textfile]\n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que hayamos leído ambos archivos, crearemos una función que recibe las dos listas (cadenas de “short_description” y “stop words”), esta función tendrá como finalidad calcular el promedio de caracteres que hay en todas las palabras, para eso se realizaron los siguientes pasos:\n",
    "\n",
    "1.- Concatenar todas las cadenas leídas del archivo JSON separándolas por un espacio, para esto utilizamos la función join.\n",
    "\n",
    "2.- La cadena obtenida anteriormente le aplicaremos la función lower para convertir en minúsculas todas las palabras de la cadena.\n",
    "\n",
    "3.- Una vez que tenemos la cadena con todas las palabras en minúsculas haremos uso de la función Split, la cual nos creará una lista con todas las palabras separadas mediante un espacio en blanco.\n",
    "\n",
    "4.- Usaremos una función map, que recibe como primer parámetro una función lambda encargada de aplicarle una expresión regular a la palabra recibida, de esta manera eliminamos caracteres que no nos sirven, como segundo parámetro del map recibimos la lista de palabras obtenida en el punto anterior.\n",
    "\n",
    "5.- Realizamos un filter con la finalidad de discriminar aquellas palabras las cuales se encuentren el la lista de “stop words”, esto lo haremos con la ayuda de una lambda y la lista obtenida. \n",
    "\n",
    "6.- Eliminaremos las cadenas vacías que quedaron después de aplicar la expresión regular a nuestra lista de palabras, para esto haremos un join combinado con Split.\n",
    "\n",
    "7.- Hacemos uso de un map para aplicarle la función len a cada palabra y así obtener una lista que contenga todos los tamaños de cada palabra.\n",
    "\n",
    "8.- Retornaremos el resultado de la función operator.truediv, la cual realiza una división y recibe como primer parámetro el resultado de la función reduce, esta recibirá la función operator.add y la lista que contiene los tamaños de las palabras, el segundo parámetro de operator.truediv será el resultado de la función len aplicada al arreglo de tamaños de las palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_words(datos,stop_words):\n",
    "    total_words = \" \".join(datos)\n",
    "    total_words_lower = total_words.lower()\n",
    "    separate_words = total_words_lower.split()\n",
    "    m = map(lambda x: re.sub(r'[^a-z0-9]','',x),separate_words)\n",
    "    leaked_words = filter(lambda x: x not in stop_words, list(m))\n",
    "    final_words = \" \".join(list(leaked_words)).split()\n",
    "    tam_words = list(map(len,final_words))\n",
    "    return operator.truediv(functools.reduce(operator.add,tam_words),len(tam_words))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta parte, mandamos a leer ambos archivos que necesitamos mediante las funciones creadas anteriormente, para una vez teniendo el resultado de estas, pasarlos como parámetro a la función encargada de calcular el promedio, al final se imprime el promedio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.657138265375316\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    l = read_json()\n",
    "    a = read_stopwords()\n",
    "    z = avg_words(l,a)\n",
    "    print(z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e8b218fb2fb742f874131db7d5423a084c076321bc52c619be06afc058ec33e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
